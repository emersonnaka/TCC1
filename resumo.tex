\begin{resumo}
\textbf{Contexto:} Em disciplinas de algoritmos ou programação, é necessário criar
e implementar uma solução para um problema. Entretanto, há a possibilidade de possuir
diversas formas de solucionar o problema e, consequentemente, formas de implementações.
Desta forma, a quantidade de implementações possíveis é vasta, dificultando a avaliação
delas pelo professor quanto ao custo, tempo e qualidade da avaliação.
Para agravar essa dificuldade, cursos massivos, abertos e \foreign{online} (MOOC)
possuem uma grande quantidade de usuários, inviabilizando
a correção individual das submissões.

\textbf{Objetivo:} O objetivo deste trabalho é propor subsídios para a avaliação de
programas submetidos em disciplinas introdutórias à computação, utilizando técnicas
de mineração e visualização de dados para construir e apresentar agrupamentos de
código-fonte semelhantes. Os subsídios propostos consistem na utilização de ferramentas
para extração de características, padronização no armazenamento dessas características
e a utilização de técnicas de agrupamento e visualização, com o auxílio de uma ferramenta.

\textbf{Método:} A primeira etapa consistiu na identificação das características que
podem ser extraídas conforme o tipo de análise utilizado: características estáticas
referentes a estilo de escrita e complexidade. Após a identificação, foi necessário o desenvolvimento de ferramentas para coletar tais medidas de forma que pudéssemos
utilizá-las para realizar a projeção e visualização. Com isso, desenvolvemos uma
ferramenta para analisar as informações disponíveis, realizando os agrupamentos e
gerando uma visualização dos programas submetidos.
Para avaliar a visualização de submissões com auxílio da ferramenta, utilizamos
uma base de dados de implementações com soluções de cinco
problemas distintos. A avaliação ocorreu em duas etapas: mediante a qualidade
das visualizações considerando as técnicas de mineração e visualização de dados; e
verificando o \foreign{feedback} da ferramenta para o professor por meio de um
questionário.

\textbf{Resultados:} Com uma base de dados de 152 implementações, obtivemos boa
avaliação da qualidade dos agrupamento. Quanto à qualidade da visualização para
fins de avaliação dos programas submetidos, realizamos um treinamento com a
apresentação dos critérios de avaliação e da ferramenta. O estudo experimental
procedeu da utilização da
ferramenta \foreign{ScienceView}, criando uma nova base de dados, e dividindo
as correções em 2 grupos: um grupo realizara a correção tradicional e o outro
utilizara a ferramenta para auxiliar na correção. Em seguida, esses dois grupos
inverteram o modo como foi realizado as correções. Ao final do estudo experimental,
os voluntários avaliaram o treinamento e a ferramenta positivamente.

\textbf{Conclusões:} 
Considerando a preservação de vizinhança, a qualidade da projeção é compatível
com outras projeções feitas com a técnica LSP e similares, apresentando bons
resultados. Em relação ao emprego de visualização para avaliação de programas,
os resultados foram limitados devido ao emprego pouco eficiente da ferramenta
e dos agrupamentos. No entanto, considerando os agrupamentos que continham
programas avaliados, existem indícios de que a visualização pode ser utilizada
com êxito e, se melhorarmos o treinamento, poderemos alcançar claramente nosso objetivo.
Ainda assim, os participantes avaliaram como positiva a
utilização da ferramenta. Como trabalho futuro, serão investigadas a utilização
de outras características do programa e o aperfeiçoamento da usabilidade e
treinamento quanto ao uso da ferramenta.

% Palavras-chaves, separadas por ponto (tente não definir mais do que cinco)
\palavraschaves{MOOC. Programação. Agrupamentos. Mineração. Visualização}
\end{resumo}



% Caso seja TCC 2, precisa traduzir o resumo e as palavras-chaves para inglês:
\begin{abstract}
\textbf{Context:} In algogrithm or programming classes, it's necessary to create and
implement a solution to a problem. However, there is the possibility of having several
ways so solve a problem and, consequently, forms of implementations. This, the amount
of possible implementations is vast, making it difficult for teachers to evaluate them
as to the cost, time and quality of assessment. Moreover, it's necessary to perform a
feedback to students regarding their implementations.To worsen this difficulty,
massive courses, open and online (\acs{MOOC}) have large numbers of users, worsing this
problem and making it impossible individual correction of submissions.

\textbf{Objective:} The objective this project is propose tools to software evaluate
submitted in introductory courses to computing, using data miniing and visualization
techniques to build and show clusters of similar source codes. Tools propose consist
of the utilization of tools for features extraction, standardization in the storage
of these features and the use clusters and visualization techniques with de aid of
a tool.

\textbf{Method:} The first step consist of identification of features that can be
extract according the analysis type used: static, style code and dynamic features.
After identification, it be necessary to develop tools to colect such measures so
that we can use them to do the cluster. Thereby, we developed a tool to mine available
information, performing clusters and gerenating a visualization of the submitted
programs. To test the tool, we have one source database with solutions of 5 different
problems. The validation will occur in two steps: through the quality of the visualizations
considering the mining techniques and data visualization; and checking the feedback
of the tool to the teacher.

\textbf{Results:} With a database of 152 implementations, we obtained good assessment
of the quality of the clusterings with loss slightly above $20\%$ for the last
projection. We performed a training with the apresentation of assessment criteria
and the tool. The experimental study was based on the use of the ScienceView,
creating a new database, and dividing the fixes in 2 groups: a group had performed
the tradicional correction and the other had used the tool to assist in the correction.
Next, this two groups reversed how corrections were made. At the end of the experimental
study, the volunteers evaluated the training and the tool positively.
\textbf{Resultados:} Com uma base de dados de 152 implementações, obtivemos boa
avaliação da qualidade dos agrupamentos com perca um pouco acima de $20\%$ para
a última projeção. Realizamos um treinamento com a apresentação dos critérios
de avaliação e da ferramenta. O estudo experimental procedeu da utilização da
ferramenta \foreign{ScienceView}, criando uma nova base de dados, e dividindo
as correções em 2 grupos: um grupo realizara a correção tradicional e o outro
utilizara a ferramenta para auxiliar na correção. Em seguida, esses dois grupos
inverteram o modo como foi realizado as correções. Ao final do estudo experimental,
os voluntários avaliaram o treinamento e a ferramenta positivamente.

\textbf{Conclusions:} 
\textbf{Conclusões:} Concluímos que, quanto maior a quantidade de dados a serem
observados, menor a perca de qualidade da projeção a fim de responder a \textbf{QP$_1$}.
E a pouca utilização da ferramenta \foreign{ScienceView} pode ter sido um dos
motivos para que respondêssemos a \textbf{QP$_2$} negativamente. Por fim,
identificamos os trabalhos futuros desse estudo a fim de torná-lo mais relevante.

% Palavras-chaves em inglês, separadas por ponto.
\keywords{MOOC. Programming. Clustering. Data mining. Visualization}
\end{abstract}
