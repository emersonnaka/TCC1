\begin{resumo}
\textbf{Contexto:} Em disciplinas de algoritmos ou programação, é necessário criar
e implementar uma solução para um problema. Entretanto, há a possibilidade de possuir
diversas formas de solucionar o problema e, consequentemente, formas de implementações.
Desta forma, a quantidade de implementações possíveis é vasta, dificultando a avaliação
delas pelo professor quanto ao custo, tempo e qualidade da avaliação.
Para agravar essa dificuldade, cursos massivos, abertos e \foreign{online} (MOOC)
possuem uma grande quantidade de usuários, inviabilizando
a correção individual das submissões.

\textbf{Objetivo:} O objetivo deste trabalho é propor subsídios para a avaliação de
programas submetidos em disciplinas introdutórias à computação, utilizando técnicas
de mineração e visualização de dados para construir e apresentar agrupamentos de
submissões semelhantes. Os subsídios propostos consistem na utilização de ferramentas
para extração de características, padronização no armazenamento dessas características
e a utilização de técnicas de agrupamento e visualização, com o auxílio de uma ferramenta.

\textbf{Método:} A primeira etapa consistiu na identificação das características que
podem ser extraídas conforme o tipo de análise utilizado: características estáticas
referentes a estilo de escrita e complexidade. Após a identificação, foi necessário
o desenvolvimento de ferramentas para coletar tais medidas de forma que pudéssemos
utilizá-las para realizar a projeção e visualização. Com isso, desenvolvemos uma
ferramenta para analisar as informações disponíveis, realizando os agrupamentos e
gerando uma visualização dos programas submetidos.
Para avaliar a visualização de submissões com auxílio da ferramenta, utilizamos
uma base de dados de implementações com soluções de cinco
problemas distintos. A avaliação ocorreu em duas etapas: mediante a qualidade
das visualizações, considerando as técnicas de mineração e visualização de dados; e
verificando o \foreign{feedback} da ferramenta para o professor por meio de um
questionário.

\textbf{Resultados:} Com uma base de dados de 152 implementações, obtivemos boa
avaliação da qualidade dos agrupamentos. Quanto à qualidade da visualização para
fins de avaliação das submissões, realizamos um treinamento com a apresentação
dos critérios de avaliação e da ferramenta. O estudo procedeu da utilização da
ferramenta \foreign{ScienceView}, criando uma nova base de dados, e dividindo
as correções em 2 grupos: um grupo realizara a correção tradicional e o outro
utilizara a ferramenta para auxiliar na correção. Em seguida, esses dois grupos
inverteram o modo como foi realizado as correções. Ao final do estudo,
os voluntários avaliaram o treinamento e a ferramenta positivamente.

\textbf{Conclusões:} 
Considerando a preservação de vizinhança, a qualidade da projeção é compatível
com outras projeções feitas com a técnica LSP e similares, apresentando resultados
similares relatados na literatura. Em relação ao emprego de visualização para
avaliação de programas, os resultados foram limitados devido ao emprego pouco
eficiente da ferramenta e dos agrupamentos. No entanto, considerando os agrupamentos
que continham programas avaliados, existem indícios de que a visualização pode
ser utilizada com êxito e, se melhorarmos o treinamento, poderemos alcançar
claramente nosso objetivo. Ainda assim, os participantes avaliaram como positiva
a utilização da ferramenta. Como trabalho futuro, serão investigadas a utilização
de outras características das submissões e o aperfeiçoamento da usabilidade e
treinamento quanto ao uso da ferramenta.

% Palavras-chaves, separadas por ponto (tente não definir mais do que cinco)
\vspace{.3cm}
\palavraschaves{MOOC. Programação. Avaliação. Agrupamentos. Mineração de dados. Visualização.}
\end{resumo}



% Caso seja TCC 2, precisa traduzir o resumo e as palavras-chaves para inglês:
\begin{abstract}
\textbf{Context:} In algorithm or programming classes, it is necessary to create and
implement a solution to a problem. However, there is the possibility of having several
ways so solve a problem and, consequently, forms of implementations. This, the amount
of possible implementations is vast, making it difficult for teachers to evaluate them
within reasonable cost, time and quality. Moreover,
massive courses, open and online (\acs{MOOC}) have large numbers of users, aggravating this
problem and making it unfeasible individual correction of submissions.

\textbf{Objective:} The objective of this project is evaluate and implement supporting tools
to assess programming assignments submitted in introductory courses to computing using
data mining and visualization techniques to build and show clusters of similar source codes. 

\textbf{Method:} The first step consisted of identification of features that can be
extracted according to static analysis and code style.
After this identification, we developed tools to colect such measures so
that we could use them for data mining and visualization. Thereby, we improved
an existing tool for performing clusters and generating visualizations of submitted
programming assingments. To evalute the tool, we have one collection of assignments'
submissions with solutions of 5 different
problems. The validation occurred in two steps: through the quality of the visualizations
considering the mining techniques and data visualization; and checking the feedback
of the tool by the teacher.

\textbf{Results:} With a database of 152 implementations, we obtained a good evaluation
of the quality of the clusters. Regarding the quality of the visualization for the purpose
of evaluating the submitted programs, we conducted an experimental study. The experimental
study was based on the use of the ScienceView tool, creating a new database, training the
study subjects with respect to the assessment criteria and tool, and assessing the
programming submissions. We organized the participants into two groups: one group
performed the traditional assessment and the other used the tool to aid in the assessment.
These two groups then switched how corrections were made. At the end of the experimental
study, the volunteers evaluated the training and the tool positively.

\textbf{Conclusions:} 
Considering the neighborhood preservation, the quality of the projection is compatible
with other projections made with the LSP technique and the like, presenting similar
results reported in the literature. In relation to the use of visualization for program
assessment, the results were limited due to the inefficient use of the tool and the
clusters. However, considering clusters containing programs evaluated by the experiment
participants, there are indications that visualization can be used successfully and, if
we improve training, we can clearly reach our goal. Nevertheless, the participants
assessed the use of the tool as positive. As future work, we will investigate the use
of other characteristics of the program and the improvement of usability and training
regarding the use of the tool.

% Palavras-chaves em inglês, separadas por ponto.
\vspace{.3cm}
\keywords{MOOC. Programming. Assessment. Clustering. Data mining. Visualization}
\end{abstract}
